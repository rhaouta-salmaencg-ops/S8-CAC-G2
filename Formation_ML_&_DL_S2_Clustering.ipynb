Titre corrig√©
<center><h1>Formation en Machine Learning et Deep Learning</h1></center>
<center><h3>Atelier 2 : Comprendre la classification non supervis√©e</h3></center>

**Objectif :** Initier les apprenants √† l‚Äôapprentissage non supervis√© √† travers l‚Äô√©tude de plusieurs algorithmes de clustering, notamment :

- **K-Means** : clustering partitionnel  
- **DBSCAN** : clustering bas√© sur la densit√©  
- **Clustering hi√©rarchique** : approche agglom√©rative  
- **Mod√®le de m√©lange gaussien (GMM - Gaussian Mixture Model)**
La classification non supervis√©e, plus pr√©cis√©ment appel√©e **apprentissage non supervis√©**, est une branche du Machine Learning dont l‚Äôobjectif est d‚Äôanalyser et de structurer des donn√©es sans √©tiquettes pr√©d√©finies (labels).

Contrairement √† l‚Äôapprentissage supervis√©, o√π les donn√©es d‚Äôentra√Ænement contiennent des √©tiquettes permettant de guider l‚Äôalgorithme, l‚Äôapprentissage non supervis√© repose uniquement sur les similarit√©s et les structures intrins√®ques pr√©sentes dans les donn√©es afin de regrouper les observations.

Dans cet atelier, nous allons explorer diff√©rentes m√©thodes de **clustering**, qui comptent parmi les techniques les plus utilis√©es en apprentissage non supervis√©.
1Ô∏è‚É£ Terminologie correcte

Remplacer :

‚ùå classification non-supervis√©e

‚ùå classification non supervis√©e

Par :

‚úÖ apprentissage non supervis√©

‚úÖ clustering (quand on parle de regroupement)
2Ô∏è‚É£ Corrections linguistiques fr√©quentes
Incorrect	Correct
machine learning	Machine Learning
deep learning	Deep Learning
non-Supervis√©e	non supervis√©e
algorithmes √† savoir	algorithmes, √† savoir
donn√©es sans labels	donn√©es sans √©tiquettes (labels)
se base sur	repose sur
motifs pr√©sents	structures ou patterns pr√©sents
3Ô∏è‚É£ Am√©lioration acad√©mique recommand√©e

Quand tu expliques un algorithme, adopte cette structure :

## Principe
Explication th√©orique claire.

## Avantages
- Point 1
- Point 2

## Inconv√©nients
- Point 1
- Point 2

Si tu veux, je peux :

‚úÖ Corriger tout le notebook proprement r√©√©crit

‚úÖ Reformuler en version plus acad√©mique (niveau rapport / PFE)

‚úÖ Simplifier pour une pr√©sentation orale

‚úÖ Am√©liorer aussi les explications math√©matiques

Dis-moi ce que tu pr√©f√®res üòä

regroupe tout dans un seul message

Voici la version compl√®te corrig√©e, harmonis√©e et reformul√©e de ton notebook, regroup√©e dans un seul message, avec correction orthographique, terminologique et am√©lioration acad√©mique.

üß† Formation en Machine Learning et Deep Learning
Atelier 2 : Comprendre le Clustering (Apprentissage Non Supervis√©)
üìå Introduction

L‚Äôapprentissage non supervis√© est une branche du Machine Learning dont l‚Äôobjectif est d‚Äôanalyser et d‚Äôextraire des structures cach√©es √† partir de donn√©es sans √©tiquettes pr√©d√©finies (labels).

Contrairement √† l‚Äôapprentissage supervis√©, o√π les donn√©es sont associ√©es √† des classes connues, l‚Äôapprentissage non supervis√© repose uniquement sur les similarit√©s intrins√®ques entre les observations afin de regrouper celles qui pr√©sentent des caract√©ristiques communes.

Parmi les techniques les plus utilis√©es en apprentissage non supervis√©, on trouve le clustering, dont le but est de partitionner les donn√©es en groupes homog√®nes appel√©s clusters.

Dans cet atelier, nous allons √©tudier quatre algorithmes majeurs :

K-Means (clustering partitionnel)

DBSCAN (clustering bas√© sur la densit√©)

Clustering hi√©rarchique (approche agglom√©rative)

Mod√®le de m√©lange gaussien (GMM - Gaussian Mixture Model)

üîµ 1. K-Means
Principe

L‚Äôalgorithme K-Means vise √† partitionner un ensemble de donn√©es en K clusters distincts.

Son fonctionnement repose sur les √©tapes suivantes :

Choisir

regroupe tous codees corriges

Voici tous les codes corrig√©s et regroup√©s proprement pour ton atelier sur le Clustering (Apprentissage Non Supervis√©).

üîπ 1Ô∏è‚É£ Importation des librairies
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
üîπ 2Ô∏è‚É£ G√©n√©ration et visualisation des donn√©es
# G√©n√©ration des donn√©es
X, y = make_blobs(n_samples=300, centers=4, cluster_std=1.2, random_state=42)

# Visualisation initiale
plt.figure()
plt.scatter(X[:, 0], X[:, 1])
plt.title("Donn√©es g√©n√©r√©es")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
üîπ 3Ô∏è‚É£ Standardisation des donn√©es
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
üîµ 4Ô∏è‚É£ K-Means
# Application de K-Means
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans_labels = kmeans.fit_predict(X_scaled)

# Visualisation
plt.figure()
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans_labels)
plt.scatter(kmeans.cluster_centers_[:, 0],
            kmeans.cluster_centers_[:, 1],
            marker='X',
            s=200)
plt.title("Clustering avec K-Means")
plt.show()

# Score de silhouette
print("Silhouette Score (K-Means):",
      silhouette_score(X_scaled, kmeans_labels))
üîµ 5Ô∏è‚É£ DBSCAN
# Application de DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)

# Visualisation
plt.figure()
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=dbscan_labels)
plt.title("Clustering avec DBSCAN")
plt.show()

# Score de silhouette (si plus d‚Äôun cluster d√©tect√©)
if len(set(dbscan_labels)) > 1:
    print("Silhouette Score (DBSCAN):",
          silhouette_score(X_scaled, dbscan_labels))
üîµ 6Ô∏è‚É£ Clustering Hi√©rarchique (Agglom√©ratif)
# Application du clustering hi√©rarchique
agglo = AgglomerativeClustering(n_clusters=4)
agglo_labels = agglo.fit_predict(X_scaled)

# Visualisation
plt.figure()
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=agglo_labels)
plt.title("Clustering Hi√©rarchique")
plt.show()

# Score de silhouette
print("Silhouette Score (Agglomerative):",
      silhouette_score(X_scaled, agglo_labels))
üîµ 7Ô∏è‚É£ Gaussian Mixture Model (GMM)
# Application du GMM
gmm = GaussianMixture(n_components=4, random_state=42)
gmm_labels = gmm.fit_predict(X_scaled)

# Visualisation
plt.figure()
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=gmm_labels)
plt.title("Clustering avec GMM")
plt.show()

# Score de silhouette
print("Silhouette Score (GMM):",
      silhouette_score(X_scaled, gmm_labels))
üîµ 8Ô∏è‚É£ M√©thode du Coude (Elbow Method)
inertia = []

for k in range(1, 11):
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(X_scaled)
    inertia.append(model.inertia_)

plt.figure()
plt.plot(range(1, 11), inertia)
plt.title("M√©thode du Coude")
plt.xlabel("Nombre de clusters")
plt.ylabel("Inertie")
plt.show()
üîµ 9Ô∏è‚É£ Comparaison des scores
print("K-Means Silhouette Score:",
      silhouette_score(X_scaled, kmeans_labels))

print("Agglomerative Silhouette Score:",
      silhouette_score(X_scaled, agglo_labels))

print("GMM Silhouette Score:",
      silhouette_score(X_scaled, gmm_labels))

if len(set(dbscan_labels)) > 1:
    print("DBSCAN Silhouette Score:",
          silhouette_score(X_scaled, dbscan_labels))
